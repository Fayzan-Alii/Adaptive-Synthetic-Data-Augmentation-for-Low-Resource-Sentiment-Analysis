{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@ anonymous urf shezad .. tum nay bhi to meri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>ab hamin kon khilae ga bana kar balke hafsa q ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>muskan sis ho bahi nahi girls behi patang urht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>koi patwari issy btay agar sabit hoa tu yeh ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>bhot acha hoa in ka sath asi hona chaiya tha l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0       neg   @ anonymous urf shezad .. tum nay bhi to meri...\n",
       "1       pos  ab hamin kon khilae ga bana kar balke hafsa q ...\n",
       "2       pos  muskan sis ho bahi nahi girls behi patang urht...\n",
       "3       neg  koi patwari issy btay agar sabit hoa tu yeh ja...\n",
       "4       neg  bhot acha hoa in ka sath asi hona chaiya tha l..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/Genai/project/Data/Dataset 11000 Reviews.tsv', sep='\\t', header=None)\n",
    "df.columns = ['sentiment', 'review']\n",
    "seed_df = df.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "seed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if synonym != word:\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n=2):\n",
    "    words = word_tokenize(sentence)\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) > 0:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_insertion(sentence, n=2):\n",
    "    words = word_tokenize(sentence)\n",
    "    for _ in range(n):\n",
    "        add_word(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def add_word(words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1 and counter < 10:\n",
    "        random_word = random.choice(words)\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "    if len(synonyms) > 0:\n",
    "        random_synonym = random.choice(synonyms)\n",
    "        insert_pos = random.randint(0, len(words))\n",
    "        words.insert(insert_pos, random_synonym)\n",
    "\n",
    "def random_swap(sentence, n=2):\n",
    "    words = word_tokenize(sentence)\n",
    "    for _ in range(n):\n",
    "        words = swap_word(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def swap_word(words):\n",
    "    if len(words) < 2:\n",
    "        return words\n",
    "    idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return words\n",
    "\n",
    "def random_deletion(sentence, p=0.1):\n",
    "    words = word_tokenize(sentence)\n",
    "    if len(words) == 1:\n",
    "        return sentence\n",
    "    new_words = [word for word in words if random.uniform(0,1) > p]\n",
    "    if len(new_words) == 0:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(seed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>synonym_replacement</th>\n",
       "      <th>random_insertion</th>\n",
       "      <th>random_swap</th>\n",
       "      <th>random_deletion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ anonymous urf shezad .. tum nay bhi to meri...</td>\n",
       "      <td>@ anonymous urf shezad .. tum nay bhi to meri ...</td>\n",
       "      <td>@ anonymous urf shezad breadbasket .. tum nay ...</td>\n",
       "      <td>@ anonymous urf shezad .. tum nay bhi to meri ...</td>\n",
       "      <td>@ urf shezad .. tum nay bhi to meri bohot izza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ab hamin kon khilae ga bana kar balke hafsa q ...</td>\n",
       "      <td>av hamin kon khilae empire state of the south ...</td>\n",
       "      <td>ab hamin kon khilae atomic number 31 ga bana k...</td>\n",
       "      <td>ab hamin kon khilae ga bana kar balke ap q trk...</td>\n",
       "      <td>ab hamin kon khilae bana kar balke hafsa q trk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>muskan sis ho bahi nahi girls behi patang urht...</td>\n",
       "      <td>muskan sis ho bahi nahi girls behi patang urht...</td>\n",
       "      <td>muskan ch'i sis ho bahi nahi girls behi patang...</td>\n",
       "      <td>bhot sis ho bahi nahi girls behi patang urhtaa...</td>\n",
       "      <td>sis ho bahi nahi girls behi patang urhtaayi me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>koi patwari issy btay agar sabit hoa tu yeh ja...</td>\n",
       "      <td>koi patwari issy btay nutrient agar sabit hoa ...</td>\n",
       "      <td>koi patwari issy shop btay agar gaol sabit hoa...</td>\n",
       "      <td>koi patwari issy btay agar sabit jail tu yeh n...</td>\n",
       "      <td>koi patwari issy agar sabit hoa tu yeh jail ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bhot acha hoa in ka sath asi hona chaiya tha l...</td>\n",
       "      <td>bhot acha hoa indium ka sath asi hona chaiya t...</td>\n",
       "      <td>bhot acha hoa in ka sath asi inch indiana hona...</td>\n",
       "      <td>bhot acha hoa tha in sath asi hona chaiya ka l...</td>\n",
       "      <td>bhot acha hoa in sath asi hona chaiya tha lant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0   @ anonymous urf shezad .. tum nay bhi to meri...   \n",
       "1  ab hamin kon khilae ga bana kar balke hafsa q ...   \n",
       "2  muskan sis ho bahi nahi girls behi patang urht...   \n",
       "3  koi patwari issy btay agar sabit hoa tu yeh ja...   \n",
       "4  bhot acha hoa in ka sath asi hona chaiya tha l...   \n",
       "\n",
       "                                 synonym_replacement  \\\n",
       "0  @ anonymous urf shezad .. tum nay bhi to meri ...   \n",
       "1  av hamin kon khilae empire state of the south ...   \n",
       "2  muskan sis ho bahi nahi girls behi patang urht...   \n",
       "3  koi patwari issy btay nutrient agar sabit hoa ...   \n",
       "4  bhot acha hoa indium ka sath asi hona chaiya t...   \n",
       "\n",
       "                                    random_insertion  \\\n",
       "0  @ anonymous urf shezad breadbasket .. tum nay ...   \n",
       "1  ab hamin kon khilae atomic number 31 ga bana k...   \n",
       "2  muskan ch'i sis ho bahi nahi girls behi patang...   \n",
       "3  koi patwari issy shop btay agar gaol sabit hoa...   \n",
       "4  bhot acha hoa in ka sath asi inch indiana hona...   \n",
       "\n",
       "                                         random_swap  \\\n",
       "0  @ anonymous urf shezad .. tum nay bhi to meri ...   \n",
       "1  ab hamin kon khilae ga bana kar balke ap q trk...   \n",
       "2  bhot sis ho bahi nahi girls behi patang urhtaa...   \n",
       "3  koi patwari issy btay agar sabit jail tu yeh n...   \n",
       "4  bhot acha hoa tha in sath asi hona chaiya ka l...   \n",
       "\n",
       "                                     random_deletion  \n",
       "0  @ urf shezad .. tum nay bhi to meri bohot izza...  \n",
       "1  ab hamin kon khilae bana kar balke hafsa q trk...  \n",
       "2  sis ho bahi nahi girls behi patang urhtaayi me...  \n",
       "3  koi patwari issy agar sabit hoa tu yeh jail ma...  \n",
       "4  bhot acha hoa in sath asi hona chaiya tha lant...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_df['synonym_replacement'] = seed_df['review'].apply(lambda x: synonym_replacement(str(x), n=2))\n",
    "seed_df['random_insertion'] = seed_df['review'].apply(lambda x: random_insertion(str(x), n=2))\n",
    "seed_df['random_swap'] = seed_df['review'].apply(lambda x: random_swap(str(x), n=2))\n",
    "seed_df['random_deletion'] = seed_df['review'].apply(lambda x: random_deletion(str(x), p=0.1))\n",
    "seed_df[['review', 'synonym_replacement', 'random_insertion', 'random_swap', 'random_deletion']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming seed_df is provided with columns: 'review', 'sentiment', 'synonym_replacement'\n",
    "paraphrase_examples = seed_df.sample(2, random_state=42)\n",
    "negative_examples = seed_df[seed_df['sentiment'] == 'neg'].sample(2, random_state=42)\n",
    "positive_examples = seed_df[seed_df['sentiment'] == 'pos'].sample(2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraphrase(text, num_return_sequences=1):\n",
    "    prompt = \"Paraphrase the following text in Roman Urdu:\\n\\n\"\n",
    "    for idx, row in paraphrase_examples.iterrows():\n",
    "        prompt += f\"Original: {row['review']}\\nParaphrase: {row['synonym_replacement']}\\n\\n\"\n",
    "    prompt += f\"Original: {text}\\nParaphrase:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    paraphrases = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return paraphrases[0] if num_return_sequences == 1 else paraphrases\n",
    "\n",
    "def generate_conditional_text(sentiment, num_return_sequences=1):\n",
    "    examples = negative_examples if sentiment == 'neg' else positive_examples\n",
    "    prompt = f\"Generate a {sentiment} sentiment text in Roman Urdu.\\n\\n\"\n",
    "    for idx, row in examples.iterrows():\n",
    "        prompt += f\"Example: {row['review']}\\n\\n\"\n",
    "    prompt += \"New text:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return texts[0] if num_return_sequences == 1 else texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df['paraphrase'] = seed_df['review'].apply(lambda x: generate_paraphrase(x))\n",
    "\n",
    "num_synthetic_per_sentiment = 50\n",
    "synthetic_neg = generate_conditional_text('neg', num_return_sequences=num_synthetic_per_sentiment)\n",
    "synthetic_pos = generate_conditional_text('pos', num_return_sequences=num_synthetic_per_sentiment)\n",
    "\n",
    "conditional_synthetic_df = pd.DataFrame({\n",
    "    'review': synthetic_neg + synthetic_pos,\n",
    "    'sentiment': ['neg'] * num_synthetic_per_sentiment + ['pos'] * num_synthetic_per_sentiment\n",
    "})\n",
    "\n",
    "synthetic_reviews = []\n",
    "synthetic_sentiments = []\n",
    "\n",
    "for col in ['synonym_replacement', 'random_insertion', 'random_swap', 'random_deletion', 'paraphrase']:\n",
    "    synthetic_reviews.extend(seed_df[col].tolist())\n",
    "    synthetic_sentiments.extend(seed_df['sentiment'].tolist())\n",
    "\n",
    "synthetic_reviews.extend(conditional_synthetic_df['review'].tolist())\n",
    "synthetic_sentiments.extend(conditional_synthetic_df['sentiment'].tolist())\n",
    "\n",
    "initial_synthetic_df = pd.DataFrame({\n",
    "    'review': synthetic_reviews,\n",
    "    'sentiment': synthetic_sentiments\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1086\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:   51778 lr:  0.000000 avg.loss:  0.153752 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "\n",
    "os.makedirs('Models/fasttext_data', exist_ok=True)\n",
    "with open('Models/fasttext_data/seed_train.txt', 'w') as f:\n",
    "    for idx, row in seed_df.iterrows():\n",
    "        label = '__label__' + row['sentiment']\n",
    "        text = row['review'].replace('\\n', ' ')\n",
    "        f.write(f\"{label} {text}\\n\")\n",
    "\n",
    "classifier = fasttext.train_supervised(input='Models/fasttext_data/seed_train.txt', epoch=25, lr=1.0, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/models/modeling_base.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:257: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1275: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  std_scores = data[\"scores\"].std()\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1302: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1305: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n",
      "  1%|          | 1/100 [00:02<03:51,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0, Reward: 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:10<03:24,  2.16s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.26 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 7/100 [00:13<02:44,  1.76s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 11/100 [00:17<02:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10, Reward: 0.7894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:29<02:33,  1.83s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.66 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 21%|██        | 21/100 [00:40<02:42,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 20, Reward: 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:43<02:58,  2.29s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 27/100 [00:49<01:52,  1.54s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.21 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 29/100 [00:53<02:18,  1.94s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 30%|███       | 30/100 [00:56<02:19,  2.00s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 31%|███       | 31/100 [00:58<02:24,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 30, Reward: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -4.26 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 41%|████      | 41/100 [01:13<00:55,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 40, Reward: 0.6146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [01:14<01:03,  1.09s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 44/100 [01:18<01:17,  1.39s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.91 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 46/100 [01:24<01:46,  1.97s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.24 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 48/100 [01:25<01:04,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 47 with very low reward: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 51%|█████     | 51/100 [01:30<01:15,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 50, Reward: 0.8186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [01:39<01:36,  2.15s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 57/100 [01:43<01:27,  2.03s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 58/100 [01:44<01:16,  1.82s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -12.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 61/100 [01:49<01:05,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 60, Reward: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 62/100 [01:53<01:25,  2.26s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 65/100 [01:59<01:18,  2.26s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.83 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 68/100 [02:03<00:50,  1.59s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -4.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 70/100 [02:06<00:48,  1.62s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 71%|███████   | 71/100 [02:08<00:46,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 70, Reward: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -16.89 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 74/100 [02:13<00:41,  1.59s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 77/100 [02:20<00:49,  2.17s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 81/100 [02:32<00:52,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 80, Reward: 0.9164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [02:40<00:25,  1.85s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 88/100 [02:41<00:14,  1.19s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.10 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 89%|████████▉ | 89/100 [02:44<00:19,  1.75s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -4.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 90/100 [02:47<00:20,  2.02s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 91/100 [02:49<00:18,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 90, Reward: 0.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 96/100 [02:57<00:07,  1.82s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [03:07<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete - Avg reward: 0.7914\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:31,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0, Reward: 0.3430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:17<03:19,  2.19s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -12.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 11/100 [00:20<02:48,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10, Reward: 0.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:22<02:41,  1.83s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.46 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 13/100 [00:24<02:48,  1.94s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 20%|██        | 20/100 [00:37<02:13,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 19 with very low reward: 0.0493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:38<01:54,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 20, Reward: 0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:56<01:36,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 30 with very low reward: 0.0279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:58<01:48,  1.60s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 33/100 [01:00<01:46,  1.59s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -4.33 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 34 with very low reward: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [01:05<01:21,  1.31s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 41%|████      | 41/100 [01:10<01:40,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 40, Reward: 0.8326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [01:15<01:31,  1.63s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 47/100 [01:22<01:57,  2.23s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.69 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 51%|█████     | 51/100 [01:29<01:15,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 50, Reward: 0.9488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 56/100 [01:39<01:19,  1.81s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 60/100 [01:46<01:04,  1.61s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 61/100 [01:46<00:55,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 60, Reward: 0.3837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [02:01<00:34,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 70, Reward: 0.9483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [02:02<00:34,  1.24s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 73/100 [02:04<00:37,  1.40s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -4.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 75/100 [02:07<00:36,  1.47s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.52 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 81/100 [02:18<00:32,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 80, Reward: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [02:34<00:12,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 90, Reward: 0.9706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [02:38<00:11,  1.71s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 95/100 [02:42<00:08,  1.78s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [02:56<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete - Avg reward: 0.7740\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0, Reward: 0.9026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:04<01:25,  1.10it/s]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 11/100 [00:15<02:48,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10, Reward: 0.5770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:28<03:02,  2.20s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 19/100 [00:30<02:24,  1.79s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.63 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 21%|██        | 21/100 [00:34<02:24,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 20, Reward: 0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 28/100 [00:48<02:03,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 27 with very low reward: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:50<01:27,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 29 with very low reward: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:51<01:26,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 30, Reward: 0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:54<01:36,  1.45s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.07 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 34/100 [00:55<01:28,  1.35s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.47 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 37/100 [01:00<01:27,  1.38s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.06 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 38/100 [01:00<01:16,  1.23s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -10.82 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 40%|████      | 40/100 [01:05<01:50,  1.84s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 41%|████      | 41/100 [01:09<02:15,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 40, Reward: 0.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [01:24<01:19,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 50, Reward: 0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [01:29<00:56,  1.25s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 61/100 [01:40<01:02,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 60, Reward: 0.9946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [01:41<00:59,  1.57s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 63/100 [01:43<00:59,  1.60s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.27 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 67/100 [01:49<00:46,  1.41s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -10.51 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 68/100 [01:50<00:42,  1.32s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.39 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 71%|███████   | 71/100 [01:55<00:39,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 70, Reward: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [02:09<00:35,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 80, Reward: 0.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -4.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 82/100 [02:11<00:29,  1.63s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.68 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 89%|████████▉ | 89/100 [02:20<00:13,  1.24s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 91/100 [02:23<00:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 90, Reward: 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [02:26<00:09,  1.38s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 99/100 [02:33<00:01,  1.40s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [02:34<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete - Avg reward: 0.8086\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:01<02:50,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0, Reward: 0.8405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:16<02:07,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 9 with very low reward: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:18<02:26,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10, Reward: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:21<02:08,  1.47s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.37 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 19/100 [00:28<01:46,  1.31s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 21%|██        | 21/100 [00:30<01:36,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 20, Reward: 0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:30<01:17,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 21 with very low reward: 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 30%|███       | 30/100 [00:41<01:31,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 29 with very low reward: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:42<01:25,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 30, Reward: 0.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:49<01:49,  1.66s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -15.30 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 38/100 [00:56<01:53,  1.83s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -6.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 41%|████      | 41/100 [01:02<02:09,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 40, Reward: 0.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [01:07<01:43,  1.85s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -8.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 49/100 [01:17<01:42,  2.01s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.40 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 51%|█████     | 51/100 [01:20<01:24,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 50, Reward: 0.8272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [01:23<01:18,  1.67s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -2.85 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 61/100 [01:41<01:15,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 60, Reward: 0.9086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [01:44<01:23,  2.19s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 68/100 [01:51<00:36,  1.15s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.64 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 71%|███████   | 71/100 [01:56<00:37,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 70, Reward: 0.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [02:04<00:40,  1.69s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -5.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 77/100 [02:08<00:52,  2.28s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 81/100 [02:17<00:41,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 80, Reward: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [02:19<00:27,  1.62s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -3.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 87/100 [02:22<00:13,  1.04s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -9.70 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 91/100 [02:29<00:13,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 90, Reward: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [02:33<00:08,  1.36s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -1.84 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 99/100 [02:43<00:01,  1.97s/it]/home/Genai/project/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1279: UserWarning: KL divergence is starting to become negative: -13.45 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete - Avg reward: 0.8199\n",
      "Training complete - 391/400 examples processed successfully\n",
      "Average reward: 0.7985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ppo_t5_small/tokenizer_config.json',\n",
       " 'ppo_t5_small/special_tokens_map.json',\n",
       " 'ppo_t5_small/spiece.model',\n",
       " 'ppo_t5_small/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2 Set Up and Train RL Loop with trl library\n",
    "from trl import PPOConfig, AutoModelForSeq2SeqLMWithValueHead, PPOTrainer\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# Ensure GPU is available and memory cleanup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Create a model with value head for PPO\n",
    "model_with_value_head = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"t5-small\")\n",
    "model_with_value_head.to(device)\n",
    "\n",
    "# PPO configuration optimized for 6GB VRAM with KL stability improvements\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=\"t5-small\",\n",
    "    learning_rate=5e-6,  # Reduced learning rate for stability\n",
    "    batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    ppo_epochs=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    kl_penalty=\"kl\",  # Add KL penalty to address negative KL\n",
    "    init_kl_coef=0.2,  # KL coefficient for stability\n",
    "    adap_kl_ctrl=True  # Adaptive KL control\n",
    ")\n",
    "\n",
    "def get_reward(text, target_sentiment):\n",
    "    # Make sure text isn't empty\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        print(\"Empty response detected, assigning zero reward\")\n",
    "        return 0.0\n",
    "        \n",
    "    try:\n",
    "        labels, probabilities = classifier.predict(text, k=2)\n",
    "        label_prob_dict = dict(zip(labels, probabilities))\n",
    "        target_label = '__label__' + target_sentiment\n",
    "        reward = label_prob_dict.get(target_label, 0.0)\n",
    "        \n",
    "        # Thorough reward sanitization\n",
    "        if not isinstance(reward, float) or math.isnan(reward) or math.isinf(reward) or reward < 0:\n",
    "            print(f\"Invalid reward ({reward}) for response: {text[:50]}...\")\n",
    "            return 0.0\n",
    "            \n",
    "        return reward\n",
    "    except Exception as e:\n",
    "        print(f\"Error in reward calculation: {str(e)}, text: {text[:50]}...\")\n",
    "        return 0.0\n",
    "\n",
    "def create_prompts(sentiment, num_prompts):\n",
    "    examples_df = seed_df[seed_df['sentiment'] == sentiment]\n",
    "    prompts = []\n",
    "    for _ in range(num_prompts):\n",
    "        examples = examples_df.sample(2)\n",
    "        prompt = f\"Generate a {sentiment} sentiment text in Roman Urdu.\\n\\n\"\n",
    "        for idx, row in examples.iterrows():\n",
    "            prompt += f\"Example: {row['review']}\\n\\n\"\n",
    "        prompt += \"New text:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "# Create dataset with the required field names for PPOTrainer\n",
    "class PPODataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, prompts, sentiments):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.prompts = prompts\n",
    "        self.sentiments = sentiments\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx], \n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"sentiment\": self.sentiments[idx]\n",
    "        }\n",
    "\n",
    "# Prepare prompts and targets\n",
    "num_prompts_per_sentiment = 50\n",
    "neg_prompts = create_prompts('neg', num_prompts_per_sentiment)\n",
    "pos_prompts = create_prompts('pos', num_prompts_per_sentiment)\n",
    "all_prompts = neg_prompts + pos_prompts\n",
    "target_sentiments = ['neg'] * num_prompts_per_sentiment + ['pos'] * num_prompts_per_sentiment\n",
    "\n",
    "# Tokenize prompts\n",
    "tokenized_prompts = tokenizer(all_prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "dataset = PPODataset(\n",
    "    tokenized_prompts['input_ids'], \n",
    "    tokenized_prompts['attention_mask'],\n",
    "    all_prompts,\n",
    "    target_sentiments\n",
    ")\n",
    "\n",
    "# Create the PPO trainer\n",
    "trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model_with_value_head,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Track successful examples for debugging\n",
    "successful_examples = 0\n",
    "total_reward = 0.0\n",
    "\n",
    "# PPO RL Loop\n",
    "for epoch in range(4):\n",
    "    print(f\"Epoch {epoch + 1}/4\")\n",
    "    epoch_reward = 0.0\n",
    "    epoch_examples = 0\n",
    "    \n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        try:\n",
    "            example = dataset[idx]\n",
    "            query_tensor = example[\"input_ids\"].unsqueeze(0).to(device)\n",
    "            attention_mask = example[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate response with more conservative parameters\n",
    "            with torch.no_grad():\n",
    "                response_tensor = model_with_value_head.generate(\n",
    "                    input_ids=query_tensor,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=100,\n",
    "                    min_length=10,  # Ensure some minimal output\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    temperature=0.9,  # Slightly higher for more exploration\n",
    "                    no_repeat_ngram_size=2  # Reduce repetition\n",
    "                )\n",
    "\n",
    "            decoded_response = tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
    "            sentiment = example[\"sentiment\"]\n",
    "            reward = get_reward(decoded_response, sentiment)\n",
    "            \n",
    "            # Skip very low rewards - they don't provide good training signal\n",
    "            if reward < 0.05:\n",
    "                print(f\"Skipping example {idx} with very low reward: {reward:.4f}\")\n",
    "                continue\n",
    "                \n",
    "            rewards_tensor = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Pass lists of 1D tensors as required by PPOTrainer\n",
    "            stats = trainer.step([query_tensor[0]], [response_tensor[0]], [rewards_tensor])\n",
    "            \n",
    "            successful_examples += 1\n",
    "            total_reward += reward\n",
    "            epoch_reward += reward\n",
    "            epoch_examples += 1\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Example {idx}, Reward: {reward:.4f}\")\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on example {idx}: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    # Print epoch stats\n",
    "    avg_epoch_reward = epoch_reward / max(1, epoch_examples)\n",
    "    print(f\"Epoch {epoch+1} complete - Avg reward: {avg_epoch_reward:.4f}\")\n",
    "    \n",
    "    # More aggressive cleanup between epochs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Final stats\n",
    "avg_reward = total_reward / max(1, successful_examples)\n",
    "print(f\"Training complete - {successful_examples}/{len(dataset)*4} examples processed successfully\")\n",
    "print(f\"Average reward: {avg_reward:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_with_value_head.pretrained_model.save_pretrained(\"Models/ppo_t5_small\")\n",
    "tokenizer.save_pretrained(\"Models/ppo_t5_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 synthetic samples. Augmented dataset size: 300\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Generate Synthetic Data with PPO-Trained Model\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the PPO-trained model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Models/ppo_t5_small\").to(\"cuda\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Models/ppo_t5_small\")\n",
    "\n",
    "def generate_synthetic_text(prompt, max_length=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def create_prompts(sentiment, num_prompts, seed_df):\n",
    "    examples_df = seed_df[seed_df['sentiment'] == sentiment]\n",
    "    prompts = []\n",
    "    for _ in range(num_prompts):\n",
    "        examples = examples_df.sample(2)\n",
    "        prompt = f\"Generate a {sentiment} sentiment text in Roman Urdu.\\n\\n\"\n",
    "        for idx, row in examples.iterrows():\n",
    "            prompt += f\"Example: {row['review']}\\n\\n\"\n",
    "        prompt += \"New text:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "# Generate synthetic data\n",
    "num_synthetic_per_sentiment = 100  # Adjust based on dataset needs\n",
    "neg_prompts = create_prompts('neg', num_synthetic_per_sentiment, seed_df)\n",
    "pos_prompts = create_prompts('pos', num_synthetic_per_sentiment, seed_df)\n",
    "\n",
    "synthetic_texts = []\n",
    "synthetic_labels = []\n",
    "\n",
    "# Generate negative sentiment texts\n",
    "for prompt in neg_prompts:\n",
    "    text = generate_synthetic_text(prompt)\n",
    "    synthetic_texts.append(text)\n",
    "    synthetic_labels.append('neg')\n",
    "\n",
    "# Generate positive sentiment texts\n",
    "for prompt in pos_prompts:\n",
    "    text = generate_synthetic_text(prompt)\n",
    "    synthetic_texts.append(text)\n",
    "    synthetic_labels.append('pos')\n",
    "\n",
    "# Create synthetic DataFrame\n",
    "synthetic_df = pd.DataFrame({\n",
    "    'review': synthetic_texts,\n",
    "    'sentiment': synthetic_labels\n",
    "})\n",
    "\n",
    "# Combine with seed data (optional, depending on your strategy)\n",
    "augmented_df = pd.concat([seed_df, synthetic_df], ignore_index=True)\n",
    "\n",
    "# Save augmented dataset\n",
    "augmented_df.to_csv('Data/augmented_dataset.csv', index=False)\n",
    "print(f\"Generated {len(synthetic_df)} synthetic samples. Augmented dataset size: {len(augmented_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading augmented dataset...\n",
      "Filtering synthetic data based on classifier confidence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 569.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 164 out of 200 synthetic examples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original augmented dataset size: 300\n",
      "Filtered augmented dataset size: 264\n"
     ]
    }
   ],
   "source": [
    "# 2.4 Filter Synthetic Data by Quality\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Loading augmented dataset...\")\n",
    "augmented_df = pd.read_csv('Data/augmented_dataset.csv')\n",
    "\n",
    "# Separate seed data from synthetic data\n",
    "seed_data = augmented_df.iloc[:len(seed_df)]\n",
    "synthetic_data = augmented_df.iloc[len(seed_df):]\n",
    "\n",
    "# Filter synthetic data based on classifier confidence\n",
    "print(\"Filtering synthetic data based on classifier confidence...\")\n",
    "high_quality_indices = []\n",
    "confidence_scores = []\n",
    "\n",
    "for idx, row in tqdm(synthetic_data.iterrows()):\n",
    "    target_label = '__label__' + row['sentiment']\n",
    "    try:\n",
    "        labels, probabilities = classifier.predict(row['review'], k=2)\n",
    "        label_prob_dict = dict(zip(labels, probabilities))\n",
    "        confidence = label_prob_dict.get(target_label, 0.0)\n",
    "        confidence_scores.append(confidence)\n",
    "        \n",
    "        # Keep only high confidence examples (0.75 threshold)\n",
    "        if confidence >= 0.75:\n",
    "            high_quality_indices.append(idx)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"Kept {len(high_quality_indices)} out of {len(synthetic_data)} synthetic examples\")\n",
    "\n",
    "# Create filtered augmented dataset\n",
    "filtered_synthetic_data = synthetic_data.loc[high_quality_indices]\n",
    "filtered_augmented_df = pd.concat([seed_data, filtered_synthetic_data], ignore_index=True)\n",
    "filtered_augmented_df.to_csv('Data/filtered_augmented_dataset.csv', index=False)\n",
    "\n",
    "# Visualize confidence distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(confidence_scores, bins=20, alpha=0.7)\n",
    "plt.axvline(x=0.75, color='r', linestyle='--', label='Threshold (0.75)')\n",
    "plt.xlabel('Classifier Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiment Classifier Confidence on Synthetic Data')\n",
    "plt.legend()\n",
    "plt.savefig('Images/synthetic_data_quality.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original augmented dataset size: {len(augmented_df)}\")\n",
    "print(f\"Filtered augmented dataset size: {len(filtered_augmented_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 14:59:46.342573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 14:59:47.278377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-11 14:59:47.610079: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-11 14:59:49.813875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-11 14:59:58.636032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adf6de222c14d77b527dddc1a25d186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cbb4b4e98e4c1494e115294da98001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ca32fdda6449e0ab0941ea84f132d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc91bac153fc4b59b08d052c4296ba8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e488aa1a98e84d4f9fc08710cb62cd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d040ab925624acda3825cda3b907395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e723a9d4ee644a01800ef7318961a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 02:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.664762</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.474191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.589400</td>\n",
       "      <td>0.621488</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.587704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>0.416851</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.848531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.4168510437011719, 'eval_accuracy': 0.85, 'eval_f1': 0.8485314685314685, 'eval_runtime': 0.5904, 'eval_samples_per_second': 101.634, 'eval_steps_per_second': 13.551, 'epoch': 3.0}\n",
      "Test Accuracy: 0.8500\n",
      "Test F1 Score: 0.8485\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Fine-Tune and Evaluate BERT\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load augmented dataset\n",
    "augmented_df = pd.read_csv('Data/augmented_dataset.csv')\n",
    "\n",
    "# Map sentiments to labels (pos: 1, neg: 0)\n",
    "augmented_df['label'] = augmented_df['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(augmented_df, test_size=0.2, stratify=augmented_df['label'], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['review', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['review', 'label']])\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2).to(\"cuda\")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./Models/bert_finetuned',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Optimized for 6GB VRAM\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True  # Mixed precision for VRAM efficiency\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('Models/bert_finetuned')\n",
    "tokenizer.save_pretrained('Models/bert_finetuned')\n",
    "\n",
    "# Predict on test set for detailed metrics\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80504aa3aa3943cdb25d5b954a29ef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb92669806ba4567a90b33a787fcd0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training with frozen layers...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 01:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.691414</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.352015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>0.687069</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.352015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.650102</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.679904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: Fine-tuning all layers...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 11:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.654142</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.720172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.622050</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.611281</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.739580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.576811</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.815897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.574209</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.832211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5742085576057434, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8322109988776655, 'eval_runtime': -0.843, 'eval_samples_per_second': -71.171, 'eval_steps_per_second': -9.489, 'epoch': 5.0}\n",
      "Test Accuracy: 0.8333\n",
      "Test F1 Score: 0.8322\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Fine-Tune and Evaluate XLM-RoBERTa with Advanced Techniques\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load filtered augmented dataset\n",
    "filtered_augmented_df = pd.read_csv('Data/augmented_dataset.csv')\n",
    "filtered_augmented_df['label'] = filtered_augmented_df['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(filtered_augmented_df, test_size=0.2, stratify=filtered_augmented_df['label'], random_state=42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use XLM-RoBERTa instead of BERT for better multilingual performance\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['review', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['review', 'label']])\n",
    "\n",
    "# Tokenize datasets with longer max length for better coverage\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "# First freeze the embedding layer and most transformer layers\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for layer in model.roberta.encoder.layer[:-2]:  # Freeze all but last 2 layers\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Stage 1 training arguments - train only top layers\n",
    "training_args_stage1 = TrainingArguments(\n",
    "    output_dir='./Models/xlm-roberta-stage1',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer_stage1 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_stage1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Stage 1: Training with frozen layers...\")\n",
    "trainer_stage1.train()\n",
    "\n",
    "# Unfreeze all layers for fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Stage 2 training arguments - fine-tune all layers with smaller learning rate\n",
    "training_args_stage2 = TrainingArguments(\n",
    "    output_dir='./Models/xlm-roberta-stage2',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-6,  # Much smaller learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Create a new trainer for stage 2\n",
    "trainer_stage2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_stage2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Stage 2: Fine-tuning all layers...\")\n",
    "trainer_stage2.train()\n",
    "\n",
    "# Final evaluation\n",
    "eval_results = trainer_stage2.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('Models/xlm-roberta-finetuned')\n",
    "tokenizer.save_pretrained('Models/xlm-roberta-finetuned')\n",
    "\n",
    "# Get detailed metrics on test set\n",
    "predictions = trainer_stage2.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Model Evaluation and Comparison with Baseline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Negative', 'Positive'],\n",
    "           yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('Images/confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Train baseline model on original seed data only\n",
    "print(\"\\nEvaluating baseline model (trained on seed data only)...\")\n",
    "\n",
    "# Create seed-only datasets\n",
    "seed_df_with_label = seed_df.copy()\n",
    "seed_df_with_label['label'] = seed_df_with_label['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "baseline_train, baseline_test = train_test_split(\n",
    "    seed_df_with_label, test_size=0.2, stratify=seed_df_with_label['label'], random_state=42\n",
    ")\n",
    "\n",
    "baseline_train_dataset = Dataset.from_pandas(baseline_train[['review', 'label']])\n",
    "baseline_test_dataset = Dataset.from_pandas(baseline_test[['review', 'label']])\n",
    "\n",
    "# Tokenize baseline datasets\n",
    "baseline_train_dataset = baseline_train_dataset.map(tokenize_function, batched=True)\n",
    "baseline_test_dataset = baseline_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "baseline_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "baseline_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create baseline model\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "# Baseline training arguments\n",
    "baseline_args = TrainingArguments(\n",
    "    output_dir='./Models/baseline-model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_args,\n",
    "    train_dataset=baseline_train_dataset,\n",
    "    eval_dataset=baseline_test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "baseline_trainer.train()\n",
    "baseline_results = baseline_trainer.evaluate()\n",
    "\n",
    "# Get baseline metrics\n",
    "baseline_predictions = baseline_trainer.predict(baseline_test_dataset)\n",
    "baseline_preds = np.argmax(baseline_predictions.predictions, axis=1)\n",
    "baseline_labels = baseline_predictions.label_ids\n",
    "baseline_accuracy = accuracy_score(baseline_labels, baseline_preds)\n",
    "baseline_f1 = f1_score(baseline_labels, baseline_preds, average='weighted')\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(f\"Baseline (seed data only) - Accuracy: {baseline_accuracy:.4f}, F1: {baseline_f1:.4f}\")\n",
    "print(f\"Enhanced (with synthetic) - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "print(f\"Improvement              - Accuracy: {accuracy - baseline_accuracy:.4f}, F1: {f1 - baseline_f1:.4f}\")\n",
    "\n",
    "# Find error cases for analysis\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted'] = preds\n",
    "test_df_with_preds['correct'] = test_df_with_preds['label'] == preds\n",
    "\n",
    "# Print a few misclassified examples\n",
    "print(\"\\n=== Error Analysis ===\")\n",
    "misclassified = test_df_with_preds[~test_df_with_preds['correct']].sample(5)\n",
    "for i, (_, row) in enumerate(misclassified.iterrows()):\n",
    "    true_sentiment = \"Positive\" if row['label'] == 1 else \"Negative\"\n",
    "    pred_sentiment = \"Positive\" if row['predicted'] == 1 else \"Negative\"\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  True sentiment: {true_sentiment}\")\n",
    "    print(f\"  Predicted sentiment: {pred_sentiment}\")\n",
    "    print(f\"  Text: {row['review'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Plot accuracy by text length\n",
    "test_df_with_preds['length'] = test_df_with_preds['review'].apply(len)\n",
    "test_df_with_preds['length_group'] = pd.qcut(test_df_with_preds['length'], 4, labels=['Very Short', 'Short', 'Medium', 'Long'])\n",
    "accuracy_by_length = test_df_with_preds.groupby('length_group')['correct'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_by_length.plot(kind='bar', color='teal')\n",
    "plt.title('Accuracy by Text Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig('Images/accuracy_by_length.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Epoch 2/4\n",
      "Epoch 3/4\n",
      "Epoch 4/4\n",
      "Saved PPO-finetuned GPT-2 model to ./ppo_gpt2_small\n"
     ]
    }
   ],
   "source": [
    "# 3.3a PPO Fine-tune GPT-2 and Save Locally\n",
    "from trl import PPOConfig, AutoModelForCausalLMWithValueHead, PPOTrainer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load GPT-2 base model and tokenizer\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Prepare PPO model\n",
    "ppo_gpt2 = AutoModelForCausalLMWithValueHead.from_pretrained(gpt2_model_name).to(device)\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=gpt2_model_name,\n",
    "    learning_rate=5e-6,\n",
    "    batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    ppo_epochs=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    kl_penalty=\"kl\",\n",
    "    init_kl_coef=0.2,\n",
    "    adap_kl_ctrl=True\n",
    ")\n",
    "\n",
    "def gpt2_create_prompts(sentiment, num_prompts, seed_df):\n",
    "    \"\"\"\n",
    "    Create prompts for GPT-2 synthetic data generation, similar to T5 prompts.\n",
    "    Each prompt includes two seed examples of the target sentiment.\n",
    "    \"\"\"\n",
    "    examples_df = seed_df[seed_df['sentiment'] == sentiment]\n",
    "    prompts = []\n",
    "    for _ in range(num_prompts):\n",
    "        examples = examples_df.sample(2)\n",
    "        prompt = f\"Generate a {sentiment} sentiment text in Roman Urdu.\\n\\n\"\n",
    "        for idx, row in examples.iterrows():\n",
    "            prompt += f\"Example: {row['review']}\\n\\n\"\n",
    "        prompt += \"New text:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "# Prepare prompts and targets (reuse your create_prompts logic)\n",
    "num_prompts_per_sentiment = 50\n",
    "neg_prompts = gpt2_create_prompts('neg', num_prompts_per_sentiment, seed_df)\n",
    "pos_prompts = gpt2_create_prompts('pos', num_prompts_per_sentiment, seed_df)\n",
    "all_prompts = neg_prompts + pos_prompts\n",
    "target_sentiments = ['neg'] * num_prompts_per_sentiment + ['pos'] * num_prompts_per_sentiment\n",
    "\n",
    "# Tokenize prompts\n",
    "tokenized_prompts = gpt2_tokenizer(all_prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "class PPODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, prompts, sentiments):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.prompts = prompts\n",
    "        self.sentiments = sentiments\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"sentiment\": self.sentiments[idx]\n",
    "        }\n",
    "\n",
    "ppo_dataset = PPODataset(\n",
    "    tokenized_prompts['input_ids'],\n",
    "    tokenized_prompts['attention_mask'],\n",
    "    all_prompts,\n",
    "    target_sentiments\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=ppo_gpt2,\n",
    "    tokenizer=gpt2_tokenizer\n",
    ")\n",
    "\n",
    "def gpt2_get_reward(text, target_sentiment):\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        labels, probabilities = classifier.predict(text, k=2)\n",
    "        label_prob_dict = dict(zip(labels, probabilities))\n",
    "        target_label = '__label__' + target_sentiment\n",
    "        reward = label_prob_dict.get(target_label, 0.0)\n",
    "        return reward if isinstance(reward, float) and reward >= 0 else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "for epoch in range(4):\n",
    "    print(f\"Epoch {epoch+1}/4\")\n",
    "    for idx in range(len(ppo_dataset)):\n",
    "        example = ppo_dataset[idx]\n",
    "        query_tensor = example[\"input_ids\"].unsqueeze(0).to(device)\n",
    "        attention_mask = example[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            response_tensor = ppo_gpt2.generate(\n",
    "                input_ids=query_tensor,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=query_tensor.shape[1] + 100,\n",
    "                min_length=10,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.9,\n",
    "                pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "            )\n",
    "        decoded_response = gpt2_tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
    "        reward = gpt2_get_reward(decoded_response, example[\"sentiment\"])\n",
    "        if reward < 0.05:\n",
    "            continue\n",
    "        rewards_tensor = torch.tensor([reward], device=device)\n",
    "        ppo_trainer.step([query_tensor[0]], [response_tensor[0]], [rewards_tensor])\n",
    "        if idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save PPO-finetuned GPT-2 model locally\n",
    "ppo_gpt2.pretrained_model.save_pretrained(\"Models/ppo_gpt2_small\")\n",
    "gpt2_tokenizer.save_pretrained(\"Models/ppo_gpt2_small\")\n",
    "print(\"Saved PPO-finetuned GPT-2 model to ./Models/ppo_gpt2_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 GPT-2 synthetic samples. Augmented dataset size: 300\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Generate Synthetic Data with PPO-Trained GPT-2 Model\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load PPO-trained GPT-2 model and tokenizer (replace with your actual PPO fine-tuned GPT-2 path)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"Models/ppo_gpt2_small\").to(\"cuda\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"Models/ppo_gpt2_small\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # For padding\n",
    "\n",
    "def gpt2_generate_synthetic_text(prompt, max_length=150):\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = gpt2_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "    )\n",
    "    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def gpt2_create_prompts(sentiment, num_prompts, seed_df):\n",
    "    examples_df = seed_df[seed_df['sentiment'] == sentiment]\n",
    "    prompts = []\n",
    "    for _ in range(num_prompts):\n",
    "        examples = examples_df.sample(2)\n",
    "        prompt = f\"Generate a {sentiment} sentiment text in Roman Urdu.\\n\\n\"\n",
    "        for idx, row in examples.iterrows():\n",
    "            prompt += f\"Example: {row['review']}\\n\\n\"\n",
    "        prompt += \"New text:\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "# Generate synthetic data\n",
    "num_synthetic_per_sentiment = 100\n",
    "neg_prompts = gpt2_create_prompts('neg', num_synthetic_per_sentiment, seed_df)\n",
    "pos_prompts = gpt2_create_prompts('pos', num_synthetic_per_sentiment, seed_df)\n",
    "\n",
    "gpt2_synthetic_texts = []\n",
    "gpt2_synthetic_labels = []\n",
    "\n",
    "for prompt in neg_prompts:\n",
    "    text = gpt2_generate_synthetic_text(prompt)\n",
    "    gpt2_synthetic_texts.append(text)\n",
    "    gpt2_synthetic_labels.append('neg')\n",
    "\n",
    "for prompt in pos_prompts:\n",
    "    text = gpt2_generate_synthetic_text(prompt)\n",
    "    gpt2_synthetic_texts.append(text)\n",
    "    gpt2_synthetic_labels.append('pos')\n",
    "\n",
    "gpt2_synthetic_df = pd.DataFrame({\n",
    "    'review': gpt2_synthetic_texts,\n",
    "    'sentiment': gpt2_synthetic_labels\n",
    "})\n",
    "\n",
    "# Combine with seed data\n",
    "gpt2_augmented_df = pd.concat([seed_df, gpt2_synthetic_df], ignore_index=True)\n",
    "gpt2_augmented_df.to_csv('Data/gpt2_augmented_dataset.csv', index=False)\n",
    "print(f\"Generated {len(gpt2_synthetic_df)} GPT-2 synthetic samples. Augmented dataset size: {len(gpt2_augmented_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 augmented dataset...\n",
      "Filtering GPT-2 synthetic data based on classifier confidence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 17105.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 0 out of 200 GPT-2 synthetic examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2 augmented dataset size: 300\n",
      "Filtered GPT-2 augmented dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Filter GPT-2 Synthetic Data by Quality\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Loading GPT-2 augmented dataset...\")\n",
    "gpt2_augmented_df = pd.read_csv('Data/gpt2_augmented_dataset.csv')\n",
    "\n",
    "# Separate seed data from synthetic data\n",
    "seed_data = gpt2_augmented_df.iloc[:len(seed_df)]\n",
    "synthetic_data = gpt2_augmented_df.iloc[len(seed_df):]\n",
    "\n",
    "print(\"Filtering GPT-2 synthetic data based on classifier confidence...\")\n",
    "high_quality_indices = []\n",
    "confidence_scores = []\n",
    "\n",
    "for idx, row in tqdm(synthetic_data.iterrows()):\n",
    "    target_label = '__label__' + row['sentiment']\n",
    "    try:\n",
    "        labels, probabilities = classifier.predict(row['review'], k=2)\n",
    "        label_prob_dict = dict(zip(labels, probabilities))\n",
    "        confidence = label_prob_dict.get(target_label, 0.0)\n",
    "        confidence_scores.append(confidence)\n",
    "        if confidence >= 0.75:\n",
    "            high_quality_indices.append(idx)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"Kept {len(high_quality_indices)} out of {len(synthetic_data)} GPT-2 synthetic examples\")\n",
    "\n",
    "filtered_synthetic_data = synthetic_data.loc[high_quality_indices]\n",
    "filtered_gpt2_augmented_df = pd.concat([seed_data, filtered_synthetic_data], ignore_index=True)\n",
    "filtered_gpt2_augmented_df.to_csv('Data/gpt2_filtered_augmented_dataset.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(confidence_scores, bins=20, alpha=0.7)\n",
    "plt.axvline(x=0.75, color='r', linestyle='--', label='Threshold (0.75)')\n",
    "plt.xlabel('Classifier Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Classifier Confidence on GPT-2 Synthetic Data')\n",
    "plt.legend()\n",
    "plt.savefig('Images/gpt2_synthetic_data_quality.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original GPT-2 augmented dataset size: {len(gpt2_augmented_df)}\")\n",
    "print(f\"Filtered GPT-2 augmented dataset size: {len(filtered_gpt2_augmented_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 12:07:14.635739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-12 12:07:15.855531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-12 12:07:16.173160: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-12 12:07:18.478183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-12 12:07:23.889133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d193809e17241f5898228061508eabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cdd5a3bb8f4dbe989f3cf1963dde9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.683044</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.542577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>0.255338</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.932884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.350974</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.864871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.25533753633499146, 'eval_accuracy': 0.9333333333333333, 'eval_f1': 0.9328843995510662, 'eval_runtime': 0.6049, 'eval_samples_per_second': 99.186, 'eval_steps_per_second': 13.225, 'epoch': 3.0}\n",
      "Test Accuracy: 0.9333\n",
      "Test F1 Score: 0.9329\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Fine-Tune and Evaluate BERT on GPT-2 Synthetic Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load GPT-2 augmented dataset\n",
    "gpt2_augmented_df = pd.read_csv('Data/gpt2_augmented_dataset.csv')\n",
    "gpt2_augmented_df['label'] = gpt2_augmented_df['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(gpt2_augmented_df, test_size=0.2, stratify=gpt2_augmented_df['label'], random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['review', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['review', 'label']])\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2).to(\"cuda\")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./Models/bert_finetuned_gpt2',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "model.save_pretrained('Models/bert_finetuned_gpt2')\n",
    "tokenizer.save_pretrained('Models/bert_finetuned_gpt2')\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83c48fda4a14a658bdcda869106499f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708bb705e01449b19eb6a3032e730afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training with frozen layers...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.688184</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.580878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>0.688184</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.687817</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.390323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./Models/xlm-roberta-gpt2-stage1/checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./Models/xlm-roberta-gpt2-stage1/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./Models/xlm-roberta-gpt2-stage1/checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: Fine-tuning all layers...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/50 02:44 < 01:57, 0.17 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>0.688208</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.580878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.688086</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./Models/xlm-roberta-gpt2-stage2/checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./Models/xlm-roberta-gpt2-stage2/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./Models/xlm-roberta-gpt2-stage2/checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6882079839706421, 'eval_accuracy': 0.65, 'eval_f1': 0.5808777429467085, 'eval_runtime': 0.3571, 'eval_samples_per_second': 56.014, 'eval_steps_per_second': 8.402, 'epoch': 3.0}\n",
      "Test Accuracy: 0.6500\n",
      "Test F1 Score: 0.5809\n"
     ]
    }
   ],
   "source": [
    "# 3.6 Fine-Tune and Evaluate XLM-RoBERTa on Filtered GPT-2 Synthetic Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load filtered GPT-2 augmented dataset\n",
    "filtered_gpt2_augmented_df = pd.read_csv('Data/gpt2_filtered_augmented_dataset.csv')\n",
    "filtered_gpt2_augmented_df['label'] = filtered_gpt2_augmented_df['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "train_df, test_df = train_test_split(filtered_gpt2_augmented_df, test_size=0.2, stratify=filtered_gpt2_augmented_df['label'], random_state=42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['review', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['review', 'label']])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "# Freeze all but last 2 layers\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in model.roberta.encoder.layer[:-2]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "training_args_stage1 = TrainingArguments(\n",
    "    output_dir='./Models/xlm-roberta-gpt2-stage1',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)\n",
    "\n",
    "trainer_stage1 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_stage1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Stage 1: Training with frozen layers...\")\n",
    "trainer_stage1.train()\n",
    "\n",
    "# Unfreeze all layers for fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "training_args_stage2 = TrainingArguments(\n",
    "    output_dir='./Models/xlm-roberta-gpt2-stage2',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./Models/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer_stage2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_stage2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Stage 2: Fine-tuning all layers...\")\n",
    "trainer_stage2.train()\n",
    "\n",
    "eval_results = trainer_stage2.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "model.save_pretrained('Models/xlm-roberta-finetuned-gpt2')\n",
    "tokenizer.save_pretrained('Models/xlm-roberta-finetuned-gpt2')\n",
    "\n",
    "predictions = trainer_stage2.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPU memory usage: 0.05 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating baseline model (seed data only, for GPT-2 comparison)...\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dde30e99dc348f09155eb108f8497cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da01bbb979644f8b8ab7ada9315648f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 02:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.702759</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.279310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.712400</td>\n",
       "      <td>0.704736</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.289333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.709644</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.362500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Genai/project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/Genai/project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Performance Comparison (GPT-2 Synthetic) ===\n",
      "Baseline (seed data only) - Accuracy: 0.4500, F1: 0.2793\n",
      "Enhanced (with GPT-2 synthetic) - Accuracy: 0.4000, F1: 0.3879\n",
      "Improvement              - Accuracy: -0.0500, F1: 0.1086\n",
      "\n",
      "=== Error Analysis (GPT-2 Synthetic) ===\n",
      "Example 1:\n",
      "  True sentiment: Positive\n",
      "  Predicted sentiment: Negative\n",
      "  Text: ya allah hamsab ko naik hidayat atta farma hamara hukumranoo ko khulfay rashideen ka tariqa par chal...\n",
      "\n",
      "Example 2:\n",
      "  True sentiment: Negative\n",
      "  Predicted sentiment: Positive\n",
      "  Text: iqbal ham sharminda hein . aap ki chuti ke qatil zinda hein . ...\n",
      "\n",
      "Example 3:\n",
      "  True sentiment: Positive\n",
      "  Predicted sentiment: Negative\n",
      "  Text: jo logh keh rhy hain k 6gb ram huni cahiye thi un jahilo ko oneplus 3t ly laina cahiye phir .. o bha...\n",
      "\n",
      "Example 4:\n",
      "  True sentiment: Positive\n",
      "  Predicted sentiment: Negative\n",
      "  Text: muskan sis ho bahi nahi girls behi patang urhtaayi hy meri friend maha ko bhot achi patang uhdni att...\n",
      "\n",
      "Example 5:\n",
      "  True sentiment: Negative\n",
      "  Predicted sentiment: Positive\n",
      "  Text: awam ma ghussay ki sb sy bri wja hukmrano ki na ehli , be rozgari taleem ki kmi , or jhalat ha , ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24133/2448992818.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  accuracy_by_length = test_df_with_preds.groupby('length_group')['correct'].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GPU memory usage: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "# 3.7 Model Evaluation and Comparison with Baseline (GPT-2 Synthetic Data)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Negative', 'Positive'],\n",
    "           yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (GPT-2 Synthetic)')\n",
    "plt.savefig('Images/confusion_matrix_gpt2.png')\n",
    "plt.show()\n",
    "\n",
    "# Baseline on seed data\n",
    "print(\"\\nEvaluating baseline model (seed data only, for GPT-2 comparison)...\")\n",
    "seed_df_with_label = seed_df.copy()\n",
    "seed_df_with_label['label'] = seed_df_with_label['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "baseline_train, baseline_test = train_test_split(\n",
    "    seed_df_with_label, test_size=0.2, stratify=seed_df_with_label['label'], random_state=42\n",
    ")\n",
    "\n",
    "baseline_train_dataset = Dataset.from_pandas(baseline_train[['review', 'label']])\n",
    "baseline_test_dataset = Dataset.from_pandas(baseline_test[['review', 'label']])\n",
    "\n",
    "baseline_train_dataset = baseline_train_dataset.map(tokenize_function, batched=True)\n",
    "baseline_test_dataset = baseline_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "baseline_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "baseline_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "baseline_args = TrainingArguments(\n",
    "    output_dir='./Models/baseline-model-gpt2',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_args,\n",
    "    train_dataset=baseline_train_dataset,\n",
    "    eval_dataset=baseline_test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "baseline_trainer.train()\n",
    "baseline_results = baseline_trainer.evaluate()\n",
    "\n",
    "baseline_predictions = baseline_trainer.predict(baseline_test_dataset)\n",
    "baseline_preds = np.argmax(baseline_predictions.predictions, axis=1)\n",
    "baseline_labels = baseline_predictions.label_ids\n",
    "baseline_accuracy = accuracy_score(baseline_labels, baseline_preds)\n",
    "baseline_f1 = f1_score(baseline_labels, baseline_preds, average='weighted')\n",
    "\n",
    "print(\"\\n=== Performance Comparison (GPT-2 Synthetic) ===\")\n",
    "print(f\"Baseline (seed data only) - Accuracy: {baseline_accuracy:.4f}, F1: {baseline_f1:.4f}\")\n",
    "print(f\"Enhanced (with GPT-2 synthetic) - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "print(f\"Improvement              - Accuracy: {accuracy - baseline_accuracy:.4f}, F1: {f1 - baseline_f1:.4f}\")\n",
    "\n",
    "# Error analysis\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted'] = preds\n",
    "test_df_with_preds['correct'] = test_df_with_preds['label'] == preds\n",
    "\n",
    "print(\"\\n=== Error Analysis (GPT-2 Synthetic) ===\")\n",
    "misclassified = test_df_with_preds[~test_df_with_preds['correct']].sample(5)\n",
    "for i, (_, row) in enumerate(misclassified.iterrows()):\n",
    "    true_sentiment = \"Positive\" if row['label'] == 1 else \"Negative\"\n",
    "    pred_sentiment = \"Positive\" if row['predicted'] == 1 else \"Negative\"\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  True sentiment: {true_sentiment}\")\n",
    "    print(f\"  Predicted sentiment: {pred_sentiment}\")\n",
    "    print(f\"  Text: {row['review'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "test_df_with_preds['length'] = test_df_with_preds['review'].apply(len)\n",
    "test_df_with_preds['length_group'] = pd.qcut(test_df_with_preds['length'], 4, labels=['Very Short', 'Short', 'Medium', 'Long'])\n",
    "accuracy_by_length = test_df_with_preds.groupby('length_group')['correct'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_by_length.plot(kind='bar', color='purple')\n",
    "plt.title('Accuracy by Text Length (GPT-2 Synthetic)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig('Images/accuracy_by_length_gpt2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 3.50 GB\n"
     ]
    }
   ],
   "source": [
    "# Code to free GPU memory completely\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete any existing model variables (add specific model variables you know exist)\n",
    "try:\n",
    "    del model, tokenizer, trainer, model_with_value_head\n",
    "    del baseline_model, baseline_trainer, ppo_gpt2, gpt2_model \n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Clear cache and collect garbage\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Check current GPU memory usage\n",
    "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
